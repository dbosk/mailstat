%& -shell-escape
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[binary,amssymb]{SIunits}
\usepackage[defblank]{paralist}

\usepackage{listings}
\lstset{%
  basicstyle=\footnotesize
}

\usepackage{noweb}
% Needed to relax penalty for breaking code chunks across pages, otherwise 
% there might be a lot of space following a code chunk.
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\smallbreak

\usepackage{csquotes}
\MakeBlockQuote{<}{ยง}{>}
\EnableQuotes

\usepackage[natbib,style=alphabetic,backend=bibtexu]{biblatex}
\addbibresource{mailstat.bib}
\addbibresource{rfc.bib}

\title{%
  mailstat: A Utility for Parsing Email Datasets
}
\author{Daniel Bosk}
\affil{%
  School of Computer Science and Communication,\\
  KTH Royal Institute of Technology, SE-100\,44 Stockholm
}
\date{Version 1.0 (draft)}

\begin{document}
\maketitle

%\begin{abstract}
%\dots
%\end{abstract}

\tableofcontents

@
\section{Introduction}

This program reads email datasets and generates statistics for them.
The kinds of statistics we are interested in are average message length and 
average number of messages per day.
We are also interested in the average number of correspondents per user.
However, to be able to get good data we want to ignore some emails, 
e.g.~newsletters, so we need some filtering functionality too.

Since these results are intended to use in papers, we want the output to be 
LaTeX code.
This way we can run this program using the [[python]] package for LaTeX to 
automatically get the figures and tables we want.

Section \ref{sec:Outline} below gives an overview of the code while 
Sect.~\ref{sec:Design} covers the main part of the algorithm.
Section \ref{sec:Output} covers the output of the results.
Finally, Sect.~\ref{sec:FutureWork} covers some future work.

\subsection{Outline}
\label{sec:Outline}

The program is a Python 3 script, [[<<mailstat.py>>]].
We will use the following structure:
<<mailstat.py>>=
#!/usr/bin/env python3
<<imports>>
<<classes>>
<<functions>>
@ We want to be able to use this code as a library, e.g.~using PythonTeX in 
a LaTeX paper.
So we will implement all parts as separate functions to provide an API for this 
purpose.
However, we also want to be able to run this as a command-line program, thus
<<mailstat.py>>=
def main( argv ):
  <<main body>>
if (__name__ == "__main__"):
  sys.exit( main( sys.argv ) )
@ The [[<<imports>>]] will contain our imported modules.
For instance, since we use [[sys.argv]] and [[sys.exit]] above we'll need to 
add
<<imports>>=
import sys
@ to [[<<imports>>]].
The code blocks [[<<classes>>]] and [[<<functions>>]] will contain our classes 
and functions, respectively.

The [[<<main body>>]] block contains the code of the main function.
Basically, we need the following code blocks:
<<main body>>=
<<parse command-line arguments>>
<<generate statistics>>
<<clean up and exit>>
@ The [[<<parse command-line arguments>>]] and [[<<parse the email dataset>>]] 
chunks are covered together, in parallel, starting in Sect.~\ref{sec:Design}.
The [[<<output result>>]] chunk is covered in Sect.~\ref{sec:Output}.

To parse command-line arguments we will make use of Python's
[[argparse]] \cite{argparse}:
<<parse command-line arguments>>=
argp = argparse.ArgumentParser( \
  description = "Generates statistics for email dataset." )
@ We also need to add it to our imports:
<<imports>>=
import argparse
@ The parsing step will then be to [[<<parse arguments>>]] and then
[[<<process arguments>>]]:
<<parse command-line arguments>>=
<<parse arguments>>
<<process arguments>>
@
The processing step is rather straight-forward using [[argparse]].
We simply parse [[argv]] and get a Python dictionary containing the variables 
we specify in [[<<parse arguments>>]]:
<<process arguments>>=
args = vars( argp.parse_args(argv[1:]) )
@


\section{Design}
\label{sec:Design}

In this section we will cover the requirements of our algorithm.
The dataset that is to be parsed with this program can be of substantial size, 
e.g.~the Enron dataset is \unit{2.6}{\giga\byte}.
Due to this we want to avoid having to keep the entire dataset in memory at the 
same time.
However, we will keep the meta-data in memory.

To store the meta-data we will use a sqlite3 database.
The default is to keep it in memory, but if we use large datasets we might want 
to be able to reuse the database.
For this reason we add a command-line argument for specifying the file name:
<<parse arguments>>=
argp.add_argument( "-f", "--file", default=":memory:", \
  help="Sets an optional file to store the database" )
@ When we create the database, we will use this file name:
<<process arguments>>=
metadata = initialize_database( args["file"] )
@ If it is not set, it will default to the special name <:memory:> which makes 
sqlite3 keep the database in memory.
The function [[initialize_database]] is defined as
<<functions>>=
def initialize_database( file ):
  metadata = sqlite3.connect( file )
  <<create database tables>>
  return metadata
@ We define what the database tables should contain later.
Of course, since we use sqlite3, we also need to import the sqlite3 module:
<<imports>>=
import sqlite3
@

Each key in the database is an attribute in the dataset.
So [[metadata]] is an index of all meta-data.
Which attributes we are interested in will be covered in 
Sect.~\ref{sec:StatisticalAnalysis}.
Next we will cover the dataset format and its parsing.

\subsection{The Dataset Format}

We want to be able to process several datasets at the same time, but we need to 
add at least one.
We will do that by adding a dataset argument to the program:
<<parse arguments>>=
argp.add_argument( "-d", "--dataset", nargs="+", \
  required=True, help="Adds email datasets (Maildir) to process" )
@ This will give us a Python list containing all the names.
Since the datasets are in Maildir format, this will be a path to the root 
directory of the Maildir structure.
As such we will use
<<imports>>=
import pathlib
@ and add each path for processing:
<<process arguments>>=
for path in args["dataset"]:
  <<update metadata>>
@

As stated above we want to read the meta-data into memory, and for this we use 
the database [[metadata]] from above.
To read all meta-data, we will traverse the directories in depth-first order 
and thus we will use a recursive function to update [[metadata]] with the data 
found in [[path]]:
<<update metadata>>=
update_metadata( metadata, pathlib.Path( path ), args["include"] )
@ We stated above that we might not want to process all messages, rather, we 
are interested in the messages in the sent folders.
To do this we add a command-line option:
<<parse arguments>>=
argp.add_argument( "-i", "--include", default=".*", \
  help="Adds a filter (regex) for inclusion of dataset files" )
@ This option adds a regular expression which must match the path.
This regular expression is passed as the [[filter]] parameter in the function 
[[update_metadata]].
This regular expression defaults to match all strings, so all paths are 
included.
We can use the following code chunk to check if a [[path]] matches the regular 
expression [[filter]]:
<<check if path matches regex>>=
pattern = re.compile( filter )
if pattern.match( str(path) ) == None:
  return metadata
@ And we must thus include the regular expressions module:
<<imports>>=
import re
@

The function [[update_metadata]] will then be defined as follows:
<<functions>>=
def update_metadata( metadata, path, filter=".*" ):
  <<check if path matches regex>>
  if path.is_dir():
    for directory_entry in path.iterdir():
      update_metadata( metadata, directory_entry )
  else:
    <<process file>>
  return metadata
@ We will iterate through the directory's contents, recursively process each 
subdirectory, and process each file.
In the Maildir format, each file is an email message.
We cover its processing next.

\subsection{The Data Format}

To parse the emails we will use the [[email]] package \cite{email} for Python 
3:
<<imports>>=
import email.message
import email.parser
@ This will allow us to parse any email message conforming to RFC 
2822~\cite{rfc2822}\footnote{%
  Note that although RFC 2822 has been obsoleted by RFC 5322~\cite{rfc5322}, 
  this is the format currently supported by the Python 3 library.
}.
We can thus parse the email file using the following function:
<<functions>>=
def parse_email( file ):
  try:
    return email.message_from_file( file )
  except:
    return None
@ And thus
<<process file>>=
email = parse_email( path.open() )
index_email( metadata, email )
@ where [[index_email]] is a function that extracts the meta-data from the 
given email and indexes it in the [[metadata]] database:
<<functions>>=
def index_email( metadata, email ):
  if not email:
    return
  <<index meta-data>>
@ What data is extracted ([[<<index meta-data>>]]) is covered in the next 
section.
When we have extracted the meta-data we index it in the meta-data storage.


\section{The Statistical Analysis}
\label{sec:StatisticalAnalysis}

In this section we will cover what statistics we are interested in, what 
meta-data we require for these, and how to compute them.
We start by covering the indexing of our dataset.
Then we continue with computing the different statistics.

\subsection{Indexing the Emails}

We want to extract the following fields from every email:
\begin{inparablank}
\item the message identifier ([[id]]),
\item who sent it ([[from]]),
\item to whom it was sent ([[to]]), including any CCs and BCCs,
\item the time it was sent ([[time]]),
\item the subject line ([[subject]]), and
\item the size of the message ([[size]]).
\end{inparablank}
This would allow us to create the following database table:
<<create database tables>>=
with metadata:
  metadata.execute( """
create table emails
  ( id TEXT,
    sender TEXT,
    recipient TEXT,
    subject TEXT,
    time TEXT,
    size INTEGER ) ;
""" )
@ The Python [[with]] statement ensures that the database commits or rolls 
back, depending whether any error occurs.

We want to index the emails using their timestamp to be able to search, sort and compute differences with the timestamps. To do this we can use the following function:
<<functions>>=
def strtounix(s):
  s = s[:-6]
  t = time.strptime(s,"%a, %d %b %Y %H:%M:%S %z")
  from time import mktime
  from datetime import datetime
  dt = datetime.fromtimestamp(mktime(t))
  return int(dt.strftime("%s"))
@
<<imports>>=
import time
@ This function first parses the string containing the date and then calculates and returns the Unix time (number of seconds that have elapsed since 00:00:00, 1st January 1970). The Unix time will then be our reference time in sorting and calculations such as average number of messages per day a user sends. The precision of the returned value by the function is in seconds, which is the best we can get from the database. The imported module provides various time-related functions as we need.

With this database we can index the email messages as follows:
<<index meta-data>>=
with metadata:
  recipients = set()
  if email["To"]:
    recipients.update( set( email["To"] ) )
  if email["CC"]:
    recipients.update( set( email["CC"] ) )
  if email["BCC"]:
    recipients.update( set( email["BCC"] ) )
  for recipient in recipients:
    metadata.execute( "insert into emails values ( ?, ?, ?, ?, ?, ? ) ;", \
    ( email["Message-ID"], email["From"], recipient, \
    email["Subject"], strtounix(email["Date"]), len( email.get_payload() ) ) )
@ The [[payload()]] method returns the message text if it is not a multi-part 
message.
If it is a multi-part message, it will return an object containing the 
different parts.
However, we leave handling of multi-part messages for future work.

\subsection{Computing Different Statistics}

We are interested in several statistics for the email dataset.
The particular statistics we are interested in are:
\begin{inparablank}
\item the mean message-size excluding any headers and attachments,
\item the mean total number of messages sent by a user,
\item the mean number of messages sent by a user per day, and
\item the mean number of contacts a user communicates with.
\end{inparablank}
In this section we will cover how to compute them.
All computations should be scientifically precise, for this we will use 
Python's [[decimal]] module:
<<imports>>=
import decimal
@

We will soon continue with defining a set of functions for computing different 
statistics.
When we later want to output these statistics running this program from the 
command-line, we will use a dictionary [[function]] to map keywords supplied on 
the command-line to these functions.
We will set this dictionary up using the [[<<keyword-to-function mapping>>]] 
chunk.
So first we let
<<keyword-to-function mapping>>=
function = {}
@
See Sect.~\ref{sec:Output} for details.

\subsubsection{The Mean Message-Size}

We will start by computing the mean message-size.
To do this we can use the following function.
<<functions>>=
def mean_message_size( metadata ):
  <<get message-sizes from the database>>
  <<return the mean with correct precision>>
@ To get the message-sizes, we can simply select those data in the meta-data 
database:
<<get message-sizes from the database>>=
values = []
with metadata as db:
  result = db.execute( "select size from emails" )
  values = list( map( lambda x: decimal.Decimal( x[0] ), result ) )
@ Now that we have the message-sizes in [[values]], and they are of the type 
[[Decimal]], we can compute the mean and standard-deviation with arbitrary 
precision.
For this we use Python's [[statistics]] module:
<<imports>>=
import statistics
@ Thus we can simply let
<<return the mean with correct precision>>=
return statistics.mean( values ), statistics.stdev( values )
@ to achieve that.

We will use a shortened version of the function name as a keyword, so we make 
the following addition
<<keyword-to-function mapping>>=
function.update( { "mean_msg_size" : mean_message_size } )
@

\subsubsection{The Mean Total Messages}

In a similar manner, we can compute the mean total number of messages per user.
To do this we define a similar function as above.
<<functions>>=
def mean_total_messages( metadata ):
  <<get the number of messages per person>>
  <<return the mean with correct precision>>
@ We already covered
[[<<return the mean with correct precision>>]].
This leaves us only with [[<<get the number of messages per person>>]] to 
define.
We let
<<get the number of messages per person>>=
values = []
with metadata as db:
  result = db.execute( "select count( sender ) from emails group by sender" )
  values = list( map( lambda x: decimal.Decimal( x[0] ), result ) )
@ This SQL query counts the number of email messages grouped by sender.
As such we could just as well apply the [[count]] function on any of the 
columns.

Finally, we add this function to the keyword mapping:
<<keyword-to-function mapping>>=
function.update( { "mean_total_msg" : mean_total_messages } )
@

\subsubsection{The Mean Message Frequency}

We continue by computing the mean number of messages a user sends per day.
This statistic requires more complex computations compared to the previous one.
To correctly estimate the message frequency per day for a user, we need to 
first find the mean frequency per day for each user, then we need to compute 
the mean for all users together.


\subsubsection{The Mean Number of Contacts}

Now we will compute the mean number of contact a user communicates with.
We do this very similarly as for the mean number of messages:
<<functions>>=
def mean_number_of_contacts( metadata ):
  <<get the number of contacts per person>>
  <<return the mean with correct precision>>
@ The only part that remains to be defined is how to fetch the correct values 
from the database.
We let
<<get the number of contacts per person>>=
values = []
with metadata as db:
  result = db.execute( """select count( distinct recipient ) from emails
    group by sender""" )
  values = list( map( lambda x: decimal.Decimal( x[0] ), result ) )
@ This SQL query counts the distinct number of recipients for the emails from 
each sender.

Finally, we add this function to the keyword mapping:
<<keyword-to-function mapping>>=
function.update( { "mean_num_contacts" : mean_number_of_contacts } )
@


\section{Generating the Output}
\label{sec:Output}

Now that we have functions to compute some statistics, we want to perform the 
computations and output them.
%We output the expected statistics as the tabular part of a LaTeX table.

We might not always be interested in all statistics.
For this reason we add a command-line argument with which we can specify which 
statistics we are interested in:
<<parse arguments>>=
argp.add_argument( "-s", "--statistics", nargs="+", \
  help="Selects statistics to output" )
@ This will give us a list of keywords to process.
For ease of use these keywords will be the names of the functions defined in 
Sect.~\ref{sec:StatisticalAnalysis} above.
Note that we do not make this option mandatory.
Sometimes we want to run the mailstat utility to just generate the sqlite3 
database file to store on disk for later use.

When we generate the output we only need to process the list of keywords, 
[[args["statistics"]]].
<<generate statistics>>=
<<keyword-to-function mapping>>
for s in args["statistics"]:
  print( "%s: %s" % ( s, function[s]( metadata ) ) )
@


\section{Future Work}
\label{sec:FutureWork}

There are things that would be worth extending in this solution.
We give a list below, somewhat in order of priority.
(Or order of complexity, so easiest first.)
\begin{itemize}
\item The database for the meta-data is probably not normalized.
However, this works for our current needs.

\item We want to be able to correctly handle multi-part messages.
Currently, text-only messages, with no attachments are assumed.

\item If the database file exists, then use that one instead of the dataset.

\item It would be useful to also have a regular expression for paths to 
exclude, currently we only have one for matching paths to include.
\end{itemize}


\section*{Acknowledgements}

This program was developed as part of my research.
Hence its development was funded by
the Swedish Foundation for Strategic Research grant SSF FFL09-0086
and the Swedish Research Council grant VR 2009-3793.

This code is available under the following MIT license:
\begin{quote}
  \input{LICENSE}
\end{quote}


\printbibliography


\section*{An Index of the Code Blocks}

\nowebchunks


\appendix
\section{An Example Email}
\label{sec:ExampleEmail}

This is an example email.
It is fetched from the Enron dataset (version 20150507), file 
<maildir/arora-h/sent/1.>.

\lstinputlisting[numbers=left]{mail.txt}


\end{document}
