%& -shell-escape
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}
\usepackage{authblk}
\usepackage[binary,amssymb]{SIunits}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{python}

\usepackage{listings}
\lstset{%
  basicstyle=\footnotesize
}

\usepackage{noweb}
% Needed to relax penalty for breaking code chunks across pages, otherwise 
% there might be a lot of space following a code chunk.
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\smallbreak

\usepackage{csquotes}
\MakeBlockQuote{<}{|}{>}
\EnableQuotes

\usepackage[natbib,style=alphabetic,backend=bibtexu]{biblatex}
\addbibresource{mailstat.bib}
\addbibresource{rfc.bib}

\title{%
  mailstat: A Utility for Parsing Email Datasets
}
\author{Daniel Bosk}
\affil{%
  School of Computer Science and Communication,\\
  KTH Royal Institute of Technology, SE-100\,44 Stockholm
}
\date{Version 1.0 (draft)}

\begin{document}
\maketitle

%\begin{abstract}
%\dots
%\end{abstract}

\tableofcontents

@
\section{Introduction}

This program reads email datasets and generates statistics for them.
The kinds of statistics we are interested in are average message length and 
average number of messages per day.
We are also interested in the average number of correspondents per user.
However, to be able to get good data we want to ignore some emails, 
e.g.~newsletters, so we need some filtering functionality too.

Since these results are intended to use in papers, we want the output to be 
LaTeX code.
This way we can run this program using the [[python]] package for LaTeX to 
automatically get the figures and tables we want.

Section \ref{sec:Outline} below gives an overview of the code while 
Sect.~\ref{sec:Design} covers the main part of the algorithm.
Section \ref{sec:Output} covers the output of the results.
Finally, Sect.~\ref{sec:FutureWork} covers some future work.

\subsection{Outline}
\label{sec:Outline}

The program is a Python 3 script, [[<<mailstat.py>>]].
We will use the following structure:
<<examgen.py>>=
#!/usr/bin/env python3
<<imports>>
<<classes>>
<<functions>>

if (__name__ == "__main__"):
  argv = sys.argv
  <<main body>>
@ Then we will successively specify what these mean.
The [[<<imports>>]] will contain our imported modules.
For instance, since we use [[sys.argv]] and [[sys.exit]] above we'll need to 
add
<<imports>>=
import sys
@ to [[<<imports>>]].
The code blocks [[<<classes>>]] and [[<<functions>>]] will contain our classes 
and functions, respectively.

The [[<<main body>>]] block contains the code of the main function.
Basically, we need the following code blocks:
<<main body>>=
<<parse command-line arguments>>
<<generate statistics>>
<<output result>>
<<clean up and exit>>
@ The [[<<parse command-line arguments>>]] and [[<<parse the email dataset>>]] 
chunks are covered together, in parallel, starting in Sect.~\ref{sec:Design}.
The [[<<output result>>]] chunk is covered in Sect.~\ref{sec:Output}.

To parse command-line arguments we will make use of Python's
[[argparse]] \cite{argparse}:
<<parse command-line arguments>>=
argp = argparse.ArgumentParser( \
  description = "Generates statistics for email dataset." )
@ We also need to add it to our imports:
<<imports>>=
import argparse
@ The parsing step will then be to [[<<parse arguments>>]] and then
[[<<process arguments>>]]:
<<parse command-line arguments>>=
<<parse arguments>>
<<process arguments>>
@
The processing step is rather straight-forward using [[argparse]].
We simply parse [[argv]] and get a Python dictionary containing the variables 
we specify in [[<<parse arguments>>]]:
<<process arguments>>=
args = vars( argp.parse_args(argv[1:]) )
@


\section{Design}
\label{sec:Design}

In this section we will cover the requirements of our algorithm.
The dataset that is to be parsed with this program can be of substantial size, 
e.g.~the Enron dataset is \unit{2.6}{\giga\byte}.
Due to this we want to avoid having to keep the entire dataset in memory at the 
same time.
However, we will keep the meta-data in memory.

\subsection{The Dataset Format}

We want to be able to process several datasets at the same time, but we need to 
add at least one.
We will do that by adding a dataset argument to the program:
<<parse arguments>>=
argp.add_argument( "-d", "--dataset", nargs="+", \
  required=True, help="Adds an email dataset (Maildir) to process" )
@ This will give us a Python list containing all the names.
Since the datasets are in Maildir format, this will be a path to the root 
directory of the Maildir structure.
As such we will use
<<imports>>=
from pathlib import Path
@ and add each path for processing:
<<process arguments>>=
paths = []
for path in args["dataset"]:
  paths.append( path )
<<process paths>>
@

As stated above we want to read the meta-data into memory.
We will store this using Python's dictionary data-structure:
<<process paths>>=
metadata = {}
@ To read all meta-data, we will traverse the directories in depth-first order 
and thus we will use the list [[paths]] as a stack:
<<process paths>>=
while len( paths ) > 0:
  path = paths.pop()
  <<process subdirectory>>
@ We will iterate through the directory's contents, add each subdirectory to 
the stack, and process each file:
<<process subdirectory>>=
for directory_entry in path.iterdir():
  if directory_entry.is_dir():
    paths.append( directory_entry )
  else:
    <<process file>>
@ In the Maildir format, each file is an email message.
We cover its processing next.

\subsection{The Data Format}

To parse the emails we will use the [[email]] package \cite{email} for Python 
3:
<<imports>>=
import email.message
import email.parser
@ This will allow us to parse any email message conforming to RFC 
2822~\cite{rfc2822}\footnote{%
  Note that although RFC 2822 has been obsoleted by RFC 5322~\cite{rfc5322}, 
  this is the format currently supported by the Python 3 library.
}.
We can thus parse the email file using the following function:
<<functions>>=
def parse_email( file ):
  parser = email.parser.Parser()
  return parser.parse( file )
@ And thus
<<process file>>=
email = parse_email( directory_entry.open() )
update_metadata( metadata, email )
@ where [[update_metadata]] is a function
<<functions>>=
def update_metadata( metadata, email ):
  <<extract meta-data>>
@ which extracts meta-data from the email.
What data is extracted is covered in the next 
section.
%Sect.~\ref{sec:StatisticalAnalysis}.


\section{The Statistical Analysis}
\label{sec:StatisticalAnalysis}

In the Enron dataset, we found that the average message was
X
excluding any headers and attachments.
The average user sent
X
messages per day, i.e.~we need on average
X
per day.
The average user communicates with
X
other users.

\dots


\section{Generating the Output}
\label{sec:Output}

Now that we have all the questions in [[exam_questions]] (see
[[<<generate exam>>]] above), we can move on to the problem of outputting it.
So in this section we define [[<<output result>>]].

The trivial solution is to just print the code for the questions to standard 
out.
Hence we let
<<output result>>=
for q in exam_questions:
  print( "%s" % q.get_code() )
@


\section{Future Work}
\label{sec:FutureWork}

There are things that would be worth extending in this solution.
We give a list below, somewhat in order of priority.
(Or order of complexity, so easiest first.)

\dots


\section*{Acknowledgements}

This program was developed as part of my research.
Hence this program's development was funded by
the Swedish Foundation for Strategic Research grant SSF FFL09-0086
and the Swedish Research Council grant VR 2009-3793.

This code is available under the following MIT license:
\begin{quote}
  \input{LICENSE}
\end{quote}


\printbibliography


\section*{An Index of the Code Blocks}

\nowebchunks


\appendix
\section{An Example Email}
\label{sec:ExampleEmail}

This is an example email.
It is fetched from the Enron dataset (version 20150507), file 
<maildir/arora-h/sent/1.>.

\lstinputlisting[numbers=left]{mail.txt}


\end{document}
