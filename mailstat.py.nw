%& -shell-escape
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[binary,amssymb]{SIunits}
\usepackage[defblank]{paralist}

\usepackage{listings}
\lstset{%
  basicstyle=\footnotesize
}

\usepackage{noweb}
% Needed to relax penalty for breaking code chunks across pages, otherwise 
% there might be a lot of space following a code chunk.
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\smallbreak

\usepackage{csquotes}
\MakeBlockQuote{<}{|}{>}
\EnableQuotes

\usepackage[natbib,style=alphabetic,backend=bibtexu]{biblatex}
\addbibresource{mailstat.bib}
\addbibresource{rfc.bib}

\title{%
  mailstat: A Utility for Parsing Email Datasets
}
\author{Daniel Bosk}
\affil{%
  School of Computer Science and Communication,\\
  KTH Royal Institute of Technology, SE-100\,44 Stockholm
}
\date{Version 1.0 (draft)}

\begin{document}
\maketitle

%\begin{abstract}
%\dots
%\end{abstract}

\tableofcontents

@
\section{Introduction}

This program reads email datasets and generates statistics for them.
The kinds of statistics we are interested in are average message length and 
average number of messages per day.
We are also interested in the average number of correspondents per user.
However, to be able to get good data we want to ignore some emails, 
e.g.~newsletters, so we need some filtering functionality too.

Since these results are intended to use in papers, we want the output to be 
LaTeX code.
This way we can run this program using the [[python]] package for LaTeX to 
automatically get the figures and tables we want.

Section \ref{sec:Outline} below gives an overview of the code while 
Sect.~\ref{sec:Design} covers the main part of the algorithm.
Section \ref{sec:Output} covers the output of the results.
Finally, Sect.~\ref{sec:FutureWork} covers some future work.

\subsection{Outline}
\label{sec:Outline}

The program is a Python 3 script, [[<<mailstat.py>>]].
We will use the following structure:
<<examgen.py>>=
#!/usr/bin/env python3
<<imports>>
<<classes>>
<<functions>>
@ We want to be able to use this code as a library, e.g.~using PythonTeX in 
a LaTeX paper.
So we will implement all parts as separate functions to provide an API for this 
purpose.
However, we also want to be able to run this as a command-line program, thus
<<examgen.py>>=
def main( argv ):
  <<main body>>
if (__name__ == "__main__"):
  sys.exit( main( sys.argv ) )
@ The [[<<imports>>]] will contain our imported modules.
For instance, since we use [[sys.argv]] and [[sys.exit]] above we'll need to 
add
<<imports>>=
import sys
@ to [[<<imports>>]].
The code blocks [[<<classes>>]] and [[<<functions>>]] will contain our classes 
and functions, respectively.

The [[<<main body>>]] block contains the code of the main function.
Basically, we need the following code blocks:
<<main body>>=
<<parse command-line arguments>>
<<generate statistics>>
<<output result>>
<<clean up and exit>>
@ The [[<<parse command-line arguments>>]] and [[<<parse the email dataset>>]] 
chunks are covered together, in parallel, starting in Sect.~\ref{sec:Design}.
The [[<<output result>>]] chunk is covered in Sect.~\ref{sec:Output}.

To parse command-line arguments we will make use of Python's
[[argparse]] \cite{argparse}:
<<parse command-line arguments>>=
argp = argparse.ArgumentParser( \
  description = "Generates statistics for email dataset." )
@ We also need to add it to our imports:
<<imports>>=
import argparse
@ The parsing step will then be to [[<<parse arguments>>]] and then
[[<<process arguments>>]]:
<<parse command-line arguments>>=
<<parse arguments>>
<<process arguments>>
@
The processing step is rather straight-forward using [[argparse]].
We simply parse [[argv]] and get a Python dictionary containing the variables 
we specify in [[<<parse arguments>>]]:
<<process arguments>>=
args = vars( argp.parse_args(argv[1:]) )
@


\section{Design}
\label{sec:Design}

In this section we will cover the requirements of our algorithm.
The dataset that is to be parsed with this program can be of substantial size, 
e.g.~the Enron dataset is \unit{2.6}{\giga\byte}.
Due to this we want to avoid having to keep the entire dataset in memory at the 
same time.
However, we will keep the meta-data in memory.

To store the meta-data we will use a sqlite3 database.
The default is to keep it in memory, but if we use large datasets we might want 
to be able to reuse the database.
For this reason we add a command-line argument for specifying the file name:
<<parse arguments>>=
argp.add_argument( "-f", "--file", default=":memory:", \
  help="Sets an optional file to store the database" )
@ When we create the database, we will use this file name:
<<process arguments>>=
metadata = sqlite3.connect( args["file"] )
<<initialize database>>
@ If it is not set, it will default to the special name <:memory:> which makes 
sqlite3 keep the database in memory.
Of course, for this we also need to import the sqlite3 module:
<<imports>>=
import sqlite3
@

Each key in the database is an attribute in the dataset.
So [[metadata]] is an index of all meta-data.
Which attributes we are interested in will be covered in 
Sect.~\ref{sec:StatisticalAnalysis}.
Next we will cover the dataset format and its parsing.

\subsection{The Dataset Format}

We want to be able to process several datasets at the same time, but we need to 
add at least one.
We will do that by adding a dataset argument to the program:
<<parse arguments>>=
argp.add_argument( "-d", "--dataset", nargs="+", \
  required=True, help="Adds email datasets (Maildir) to process" )
@ This will give us a Python list containing all the names.
Since the datasets are in Maildir format, this will be a path to the root 
directory of the Maildir structure.
As such we will use
<<imports>>=
from pathlib import Path
@ and add each path for processing:
<<process arguments>>=
for path in args["dataset"]:
  <<update metadata>>
@

As stated above we want to read the meta-data into memory, and for this we use 
the database [[metadata]] from above.
To read all meta-data, we will traverse the directories in depth-first order 
and thus we will use a recursive function to update [[metadata]] with the data 
found in [[path]]:
<<update metadata>>=
update_metadata( metadata, path, args["include"] )
@ We stated above that we might not want to process all messages, rather, we 
are interested in the messages in the sent folders.
To do this we add a command-line option:
<<parse arguments>>=
argp.add_argument( "-i", "--include", default=".*", \
  help="Adds a filter (regex) for inclusion of dataset files" )
@ This option adds a regular expression which must match the path.
This regular expression is passed as the [[filter]] parameter in the function 
[[update_metadata]].
This regular expression defaults to match all strings, so all paths are 
included.
We can use the following code chunk to check if a [[path]] matches the regular 
expression [[filter]]:
<<check if path matches regex>>=
pattern = re.compile( filter )
if pattern.match( str(path) ) == None:
  return metadata
@ And we must thus include the regular expressions module:
<<imports>>=
import re
@

The function [[update_metadata]] will then be defined as follows:
<<functions>>=
def update_metadata( metadata, path, filter=".*" ):
  <<check if path matches regex>>
  if path.is_dir():
    for directory_entry in path.iterdir():
      return update_metadata( metadata, directory_entry )
  else
    <<process file>>
  return metadata
@ We will iterate through the directory's contents, recursively process each 
subdirectory, and process each file.
In the Maildir format, each file is an email message.
We cover its processing next.

\subsection{The Data Format}

To parse the emails we will use the [[email]] package \cite{email} for Python 
3:
<<imports>>=
import email.message
import email.parser
@ This will allow us to parse any email message conforming to RFC 
2822~\cite{rfc2822}\footnote{%
  Note that although RFC 2822 has been obsoleted by RFC 5322~\cite{rfc5322}, 
  this is the format currently supported by the Python 3 library.
}.
We can thus parse the email file using the following function:
<<functions>>=
def parse_email( file ):
  parser = email.parser.Parser()
  return parser.parse( file )
@ And thus
<<process file>>=
email = parse_email( path.open() )
index_email( metadata, email )
@ where [[index_metadata]] is a function that extracts the meta-data from the 
given email and indexes it in the [[metadata]] database:
<<functions>>=
def index_email( metadata, email ):
  <<index meta-data>>
@ What data is extracted ([[<<index meta-data>>]]) is covered in the next 
section.
When we have extracted the meta-data we index it in the meta-data storage.


\section{The Statistical Analysis}
\label{sec:StatisticalAnalysis}

In this section we will cover what statistics we are interested in and what 
meta-data we require for these, i.e.~[[<<generate statistics>>]] and
[[<<index meta-data>>]], respectively.
This also lets us cover [[<<initialize database>>]].

\subsection{Indexing the Emails}

We want to extract the following fields from every email:
\begin{inparablank}
\item the message identifier ([[id]]),
\item who sent it ([[from]]),
\item to whom it was sent ([[to]]), including any CC ([[cc]]) and BCC 
([[bcc]]),
\item the time it was sent ([[time]]),
\item the subject line ([[subject]]), and
\item the size of the message ([[size]]).
\end{inparablank}
This would allow us to create the following database table:
<<initialize database>>=
with metadata:
  metadata.execute( """
create table emails
  ( id TEXT,
    from TEXT,
    to TEXT, cc TEXT, bcc TEXT,
    subject TEXT,
    time TEXT,
    size INTEGER ) ;
""" )
@ The Python [[with]] statement ensures that the database commits or rolls 
back, depending whether any error occurs.

With this database we can index the email messages as follows:
<<index meta-data>>=
with metadata:
  metadata.execute( "insert into emails values ( ?, ?, ?, ?, ?, ? ) ;", \
    email["Message-ID"], email["From"], \
    email["To"], email["CC"], email["BCC"], \
    email["Subject"], \
    email["Date"], len( email.get_payload() ) )
@ The [[payload()]] method returns the message text if it is not a multi-part 
message.
If it is a multi-part message, it will return an object containing the 
different parts.
However, we leave handling of multi-part messages for future work.

\subsection{Computing Different Statistics}

We are interested in several statistics for the email dataset.
In this section we will cover how to compute them.
The statistics we are interested in are:
\begin{inparablank}
\item the average message-size excluding any headers and attachments,
\item the average number of messages per day sent by a user, and
\item the average number of contacts a user communicates with.
\end{inparablank}

We will start by computing the average message-size.
To do this we can use the following function.
<<functions>>=
def average_message_size( metadata ):
  <<compute the average message-size>>
  <<compute the standard-deviation of message-sizes>>
  <<return the average message size with correct precision>>
@


\section{Generating the Output}
\label{sec:Output}

Now that we have some statistics computed, we want to output them.
We output the expected statistics as the tabular part of a LaTeX table.


\section{Future Work}
\label{sec:FutureWork}

There are things that would be worth extending in this solution.
We give a list below, somewhat in order of priority.
(Or order of complexity, so easiest first.)
\begin{itemize}
\item The database for the meta-data is probably not normalized.
However, this works for our current needs.

\item We want to be able to correctly handle multi-part messages.
Currently, text-only messages, with no attachments are assumed.

\item If the database file exists, then use that one instead of the dataset.
\end{itemize}


\section*{Acknowledgements}

This program was developed as part of my research.
Hence this program's development was funded by
the Swedish Foundation for Strategic Research grant SSF FFL09-0086
and the Swedish Research Council grant VR 2009-3793.

This code is available under the following MIT license:
\begin{quote}
  \input{LICENSE}
\end{quote}


\printbibliography


\section*{An Index of the Code Blocks}

\nowebchunks


\appendix
\section{An Example Email}
\label{sec:ExampleEmail}

This is an example email.
It is fetched from the Enron dataset (version 20150507), file 
<maildir/arora-h/sent/1.>.

\lstinputlisting[numbers=left]{mail.txt}


\end{document}
